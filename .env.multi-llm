# Multi-LLM Backend Configuration
# Copy this to .env and uncomment/modify the backend you want to use

# =============================================================================
# LLM Backend Selection
# =============================================================================
# Options: ollama, openai, anthropic, textgen, localai, huggingface
LLM_BACKEND=ollama

# =============================================================================
# Ollama Configuration (Current Default)
# =============================================================================
OLLAMA_MODEL=mistral:7b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.1

# Alternative Ollama models to try:
# OLLAMA_MODEL=llama2:7b
# OLLAMA_MODEL=codellama:7b
# OLLAMA_MODEL=phi3:mini
# OLLAMA_MODEL=gemma:7b

# =============================================================================
# OpenAI Configuration
# =============================================================================
# OPENAI_API_KEY=your-openai-api-key-here
# OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_TEMPERATURE=0.1
# OPENAI_MAX_TOKENS=2000

# Alternative OpenAI models:
# OPENAI_MODEL=gpt-4
# OPENAI_MODEL=gpt-4-turbo-preview
# OPENAI_MODEL=gpt-3.5-turbo-16k

# =============================================================================
# Anthropic Claude Configuration
# =============================================================================
# ANTHROPIC_API_KEY=your-anthropic-api-key-here
# ANTHROPIC_MODEL=claude-3-haiku-20240307
# ANTHROPIC_TEMPERATURE=0.1
# ANTHROPIC_MAX_TOKENS=2000

# Alternative Anthropic models:
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_MODEL=claude-3-opus-20240229

# =============================================================================
# text-generation-webui Configuration
# =============================================================================
# TEXTGEN_BASE_URL=http://localhost:5000
# TEXTGEN_MODEL=your-model-name
# TEXTGEN_TEMPERATURE=0.1
# TEXTGEN_MAX_TOKENS=2000

# =============================================================================
# LocalAI Configuration  
# =============================================================================
# LOCALAI_BASE_URL=http://localhost:8080
# LOCALAI_MODEL=your-model-name
# LOCALAI_TEMPERATURE=0.1

# =============================================================================
# Hugging Face Configuration
# =============================================================================
# HUGGINGFACE_API_KEY=your-hf-api-key-here
# HUGGINGFACE_MODEL=microsoft/DialoGPT-large
# HUGGINGFACE_TEMPERATURE=0.1

# =============================================================================
# Performance and Caching Settings
# =============================================================================
# Enable response caching to improve performance
ENABLE_LLM_CACHE=true
LLM_CACHE_TTL=3600

# Maximum context length (adjust based on model)
MAX_CONTEXT_LENGTH=4096

# Timeout settings (seconds)
LLM_REQUEST_TIMEOUT=60
LLM_MAX_RETRIES=3

# =============================================================================
# Hybrid Configuration (Advanced)
# =============================================================================
# Use different models for different tasks
# ENABLE_TASK_ROUTING=false
# TASK_ROUTING_CONFIG={
#     "simple_qa": "ollama:mistral:7b",
#     "complex_reasoning": "openai:gpt-4", 
#     "code_analysis": "ollama:codellama:7b",
#     "document_summary": "anthropic:claude-3-haiku"
# }

# Fallback configuration
# ENABLE_FALLBACK=false
# PRIMARY_BACKEND=ollama
# FALLBACK_BACKEND=openai

# =============================================================================
# Cost Management (for Cloud APIs)
# =============================================================================
# Maximum daily cost in USD (prevents runaway costs)
# MAX_DAILY_COST=10.00
# COST_TRACKING_ENABLED=false

# =============================================================================
# Model-Specific Optimizations
# =============================================================================
# Adjust these based on your chosen model's capabilities

# For code-focused models (CodeLlama, DeepSeek Coder)
# CODE_MODEL_OPTIMIZATIONS=true

# For reasoning-heavy models (GPT-4, Claude)
# ENABLE_CHAIN_OF_THOUGHT=true

# For efficiency-focused models (Phi-3, Gemma)
# OPTIMIZE_FOR_SPEED=true

# =============================================================================
# RAG Configuration
# =============================================================================
# These work with all backends
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=all-MiniLM-L6-v2
VECTOR_DB_COLLECTION=aidiot_docs

# =============================================================================
# OCR Configuration  
# =============================================================================
TESSERACT_PATH=auto-detect
DPI=300

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=127.0.0.1
API_PORT=8000

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s